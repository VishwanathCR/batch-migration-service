Nice — below is a complete, production-ready pattern you can drop into a Gradle + Java 21 project that uses **Spring Batch 5.2** to:

* take a **dynamic SQL file** (the SELECT query) as a job parameter,
* take a **parameter-query** (a query that returns rows of parameters) as a job parameter,
* spawn a **partition** per parameter row (scales over partitions / threads),
* run the dynamic SQL inside each partition with that parameter row bound,
* write partition outputs to per-partition CSV files and **merge** them into a single clean CSV (header + rows + optional footer with total count),
* works with DB2 via JDBC (uses Spring’s `JdbcTemplate` / `NamedParameterJdbcTemplate`).

I intentionally used `Map<String,Object>` for rows (flexible for arbitrary select lists). The writer builds the CSV header dynamically from the first row’s keys and escapes fields safely.

---

# How it works (quick)

1. Job parameters:

   * `sqlFile` — path to a file containing the SELECT SQL (can include named parameters like `:customer_id`, `:region`, etc).
   * `paramSelect` — SQL that returns rows of parameter values. Each row becomes one partition. Example `SELECT customer_id FROM PARAM_TABLE WHERE active = 1`.
   * `outputDir` — directory to write CSVs (default `./batch-output`).
2. Partitioner executes `paramSelect` and creates an ExecutionContext per parameter row with a `paramMap`.
3. Worker step (partition) runs a step-scoped `NamedParameterJdbcTemplate` reader that executes the dynamic SQL using the `paramMap` and emits `Map<String,Object>` rows.
4. Each partition writes to `outputDir/output-part-{partitionId}.csv`.
5. `JobCompletionListener` merges partition CSVs into `outputDir/final-output.csv` and writes a footer with total rows (optional).

---

# Project files

Below are the essential files. Keep package `com.example.batch` (change if you like).

### `build.gradle.kts`

```kotlin
plugins {
    java
    id("org.springframework.boot") version "3.2.0"
    id("io.spring.dependency-management") version "1.1.0"
}

group = "com.example"
version = "0.0.1-SNAPSHOT"
java {
    sourceCompatibility = JavaVersion.VERSION_21
    targetCompatibility = JavaVersion.VERSION_21
}

repositories {
    mavenCentral()
}

dependencies {
    implementation("org.springframework.boot:spring-boot-starter-batch")
    implementation("org.springframework.boot:spring-boot-starter-jdbc")
    implementation("com.ibm.db2:jcc") // DB2 JDBC driver artifact coordinate — adjust as needed
    implementation("org.slf4j:slf4j-api:2.0.7")
    implementation("ch.qos.logback:logback-classic:1.4.11")

    testImplementation("org.springframework.boot:spring-boot-starter-test")
    testImplementation("org.springframework.batch:spring-batch-test")
}
```

> **Note:** Add the correct DB2 driver dependency (groupId/artifactId version) for your environment or place the driver jar in the application classpath as appropriate.

---

### `src/main/resources/application.yml`

```yaml
spring:
  datasource:
    url: jdbc:db2://localhost:50000/DBNAME   # set your DB2 URL
    username: dbuser
    password: dbpass
    driver-class-name: com.ibm.db2.jcc.DB2Driver

logging:
  level:
    root: INFO
    org.springframework.batch: INFO
```

---

### `src/main/java/com/example/batch/BatchApplication.java`

```java
package com.example.batch;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class BatchApplication {
    public static void main(String[] args) {
        SpringApplication.run(BatchApplication.class, args);
    }
}
```

---

### `src/main/java/com/example/batch/config/BatchConfig.java`

```java
package com.example.batch.config;

import com.example.batch.partition.ParamPartitioner;
import com.example.batch.reader.DynamicSqlItemReader;
import com.example.batch.writer.MapCsvItemWriter;
import com.example.batch.listener.MergeFilesJobListener;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.batch.core.*;
import org.springframework.batch.core.configuration.annotation.*;
import org.springframework.batch.core.launch.support.RunIdIncrementer;
import org.springframework.batch.core.partition.support.TaskExecutorPartitionHandler;
import org.springframework.batch.core.step.tasklet.MethodInvokingTaskletAdapter;
import org.springframework.batch.item.ItemProcessor;
import org.springframework.batch.item.ItemStreamReader;
import org.springframework.batch.item.support.CompositeItemWriter;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.*;
import org.springframework.core.io.ResourceLoader;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;

import javax.sql.DataSource;
import java.io.IOException;
import java.nio.file.*;
import java.util.*;

@Configuration
@EnableBatchProcessing
public class BatchConfig {

    private static final Logger log = LoggerFactory.getLogger(BatchConfig.class);

    @Autowired
    private JobBuilderFactory jobs;

    @Autowired
    private StepBuilderFactory steps;

    @Autowired
    private DataSource dataSource;

    @Autowired
    private ResourceLoader resourceLoader;

    @Bean
    public ParamPartitioner partitioner(JdbcTemplate jdbcTemplate,
                                        @Value("#{jobParameters['paramSelect']}") String paramSelect) {
        return new ParamPartitioner(jdbcTemplate, paramSelect);
    }

    @Bean
    public ThreadPoolTaskExecutor partitionTaskExecutor(@Value("${batch.partition.threads:5}") int threads) {
        ThreadPoolTaskExecutor te = new ThreadPoolTaskExecutor();
        te.setCorePoolSize(threads);
        te.setMaxPoolSize(threads);
        te.setThreadNamePrefix("batch-part-");
        te.initialize();
        return te;
    }

    @Bean
    public TaskExecutorPartitionHandler partitionHandler(ThreadPoolTaskExecutor taskExecutor,
                                                          Step slaveStep,
                                                          @Value("${batch.partition.gridSize:10}") int gridSize) {
        TaskExecutorPartitionHandler handler = new TaskExecutorPartitionHandler();
        handler.setTaskExecutor(taskExecutor);
        handler.setStep(slaveStep);
        handler.setGridSize(gridSize);
        return handler;
    }

    @Bean
    public Step masterStep(ParamPartitioner partitioner,
                           TaskExecutorPartitionHandler partitionHandler) {
        return steps.get("masterStep")
                .partitioner("slaveStep", partitioner)
                .partitionHandler(partitionHandler)
                .build();
    }

    // Slave step: chunk oriented; reader & writer are step-scoped and created in separate beans
    @Bean
    public Step slaveStep(ItemStreamReader<Map<String, Object>> reader,
                          MapCsvItemWriter writer) {
        return steps.get("slaveStep")
                .<Map<String, Object>, Map<String, Object>>chunk(500)
                .reader(reader)
                .writer(writer)
                .build();
    }

    @Bean
    @StepScope
    public ItemStreamReader<Map<String, Object>> reader(
            DataSource ds,
            @Value("#{jobParameters['sqlFile']}") String sqlFile,
            @Value("#{stepExecutionContext['paramMap']}") Map<String, Object> paramMap) throws IOException {

        // read SQL file contents
        String sql = Files.readString(Path.of(sqlFile));
        return new DynamicSqlItemReader(ds, sql, paramMap);
    }

    @Bean
    @StepScope
    public MapCsvItemWriter writer(@Value("#{stepExecutionContext['partitionId']}") String partitionId,
                                   @Value("#{jobParameters['outputDir'] ?: './batch-output'}") String outputDir) {
        Path outDir = Paths.get(outputDir);
        try {
            Files.createDirectories(outDir);
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
        Path file = outDir.resolve("output-part-" + partitionId + ".csv");
        return new MapCsvItemWriter(file);
    }

    @Bean
    public Job job(Step masterStep,
                   MergeFilesJobListener listener) {
        return jobs.get("dynamicSqlPartitionJob")
                .incrementer(new RunIdIncrementer())
                .start(masterStep)
                .listener(listener)
                .build();
    }

    @Bean
    public MergeFilesJobListener mergeFilesJobListener(@Value("#{jobParameters['outputDir'] ?: './batch-output'}") String outputDir) {
        return new MergeFilesJobListener(Paths.get(outputDir), Paths.get(outputDir).resolve("final-output.csv"));
    }
}
```

---

### `src/main/java/com/example/batch/partition/ParamPartitioner.java`

```java
package com.example.batch.partition;

import org.springframework.batch.core.partition.support.Partitioner;
import org.springframework.batch.item.ExecutionContext;
import org.springframework.jdbc.core.JdbcTemplate;

import java.util.*;

/**
 * Reads parameter rows from the database using paramSelect and
 * creates one partition per row. Each partition's ExecutionContext contains:
 * - "partitionId" : unique id
 * - "paramMap" : Map<String,Object> of column -> value
 */
public class ParamPartitioner implements Partitioner {

    private final JdbcTemplate jdbcTemplate;
    private final String paramSelect;

    public ParamPartitioner(JdbcTemplate jdbcTemplate, String paramSelect) {
        this.jdbcTemplate = jdbcTemplate;
        this.paramSelect = paramSelect;
    }

    @Override
    public Map<String, ExecutionContext> partition(int gridSize) {
        List<Map<String, Object>> params = jdbcTemplate.queryForList(paramSelect);
        Map<String, ExecutionContext> result = new HashMap<>();
        int i = 0;
        for (Map<String, Object> row : params) {
            ExecutionContext ctx = new ExecutionContext();
            ctx.putString("partitionId", Integer.toString(i));
            // ExecutionContext requires serializable values - the map from JDBC should be serializable
            ctx.put("paramMap", new HashMap<>(row));
            result.put("partition" + i, ctx);
            i++;
        }
        if (result.isEmpty()) {
            // create a single empty partition to avoid job failing
            ExecutionContext ctx = new ExecutionContext();
            ctx.putString("partitionId", "0");
            ctx.put("paramMap", new HashMap<String, Object>());
            result.put("partition0", ctx);
        }
        return result;
    }
}
```

---

### `src/main/java/com/example/batch/reader/DynamicSqlItemReader.java`

```java
package com.example.batch.reader;

import org.springframework.batch.item.*;

import org.springframework.jdbc.core.*;
import org.springframework.jdbc.support.JdbcUtils;

import javax.sql.DataSource;
import java.sql.*;
import java.util.*;

/**
 * Simple step scoped reader that runs the provided SQL using NamedParameter binding
 * from paramMap and returns each row as Map<String,Object>.
 *
 * It's implemented as an ItemStreamReader so Spring Batch can manage open/close.
 */
public class DynamicSqlItemReader implements ItemStreamReader<Map<String, Object>> {

    private final NamedParameterJdbcTemplate namedJdbc;
    private final String sql;
    private final Map<String, Object> paramMap;

    private Iterator<Map<String, Object>> iterator;

    public DynamicSqlItemReader(DataSource ds, String sql, Map<String, Object> paramMap) {
        this.namedJdbc = new NamedParameterJdbcTemplate(ds);
        this.sql = sql;
        this.paramMap = (paramMap == null) ? Collections.emptyMap() : paramMap;
    }

    @Override
    public Map<String, Object> read() throws Exception {
        if (iterator == null) {
            // lazy execute
            List<Map<String, Object>> rows = namedJdbc.query(sql, paramMap, (rs, rowNum) -> {
                ResultSetMetaData md = rs.getMetaData();
                Map<String, Object> map = new LinkedHashMap<>();
                for (int i = 1; i <= md.getColumnCount(); i++) {
                    String col = JdbcUtils.lookupColumnName(md, i);
                    map.put(col, rs.getObject(i));
                }
                return map;
            });
            this.iterator = rows.iterator();
        }
        if (iterator != null && iterator.hasNext()) {
            return iterator.next();
        } else {
            return null;
        }
    }

    @Override
    public void open(ExecutionContext executionContext) throws ItemStreamException { }

    @Override
    public void update(ExecutionContext executionContext) throws ItemStreamException { }

    @Override
    public void close() throws ItemStreamException { }
}
```

---

### `src/main/java/com/example/batch/writer/MapCsvItemWriter.java`

```java
package com.example.batch.writer;

import org.springframework.batch.item.ItemWriter;

import java.io.*;
import java.nio.file.*;
import java.util.*;
import java.util.concurrent.atomic.AtomicBoolean;

/**
 * Writes Maps to a CSV file. The header is derived from the first item's keys (in insertion order).
 * This writer is not thread-shared; it's step-scoped and each partition gets its own file.
 */
public class MapCsvItemWriter implements ItemWriter<Map<String, Object>>, Closeable {

    private final Path file;
    private final BufferedWriter writer;
    private final AtomicBoolean headerWritten = new AtomicBoolean(false);
    private List<String> headerOrder;

    public MapCsvItemWriter(Path file) {
        this.file = file;
        try {
            this.writer = Files.newBufferedWriter(file, StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING);
        } catch (IOException e) {
            throw new UncheckedIOException(e);
        }
    }

    @Override
    public void write(List<? extends Map<String, Object>> items) throws Exception {
        if (items.isEmpty()) return;
        if (!headerWritten.getAndSet(true)) {
            // derive header from first item
            Map<String, Object> first = items.get(0);
            headerOrder = new ArrayList<>(first.keySet());
            writer.write(String.join(",", escapeList(headerOrder)));
            writer.newLine();
        }

        for (Map<String, Object> row : items) {
            // Ensure consistent column order
            StringBuilder sb = new StringBuilder();
            for (int i = 0; i < headerOrder.size(); i++) {
                String key = headerOrder.get(i);
                Object val = row.get(key);
                sb.append(escape(fieldString(val)));
                if (i < headerOrder.size() - 1) sb.append(',');
            }
            writer.write(sb.toString());
            writer.newLine();
        }
        writer.flush();
    }

    private static String fieldString(Object o) {
        return (o == null) ? "" : o.toString();
    }

    private static String escape(String s) {
        // Minimal CSV escaping: wrap in quotes if contains comma, quote or newline; double quotes inside
        if (s.indexOf('"') >= 0) {
            s = s.replace("\"", "\"\"");
        }
        if (s.indexOf(',') >= 0 || s.indexOf('"') >= 0 || s.indexOf('\n') >= 0 || s.indexOf('\r') >= 0) {
            return "\"" + s + "\"";
        }
        return s;
    }

    private static List<String> escapeList(List<String> xs) {
        List<String> out = new ArrayList<>(xs.size());
        for (String s : xs) out.add(escape(s));
        return out;
    }

    @Override
    public void close() throws IOException {
        writer.close();
    }

    public Path getFile() {
        return file;
    }
}
```

---

### `src/main/java/com/example/batch/listener/MergeFilesJobListener.java`

```java
package com.example.batch.listener;

import org.slf4j.*;
import org.springframework.batch.core.*;
import org.springframework.batch.core.listener.JobExecutionListenerSupport;

import java.io.*;
import java.nio.file.*;
import java.util.*;
import java.util.stream.Collectors;

/**
 * After job completes, merges all "output-part-*.csv" files into a single final file.
 * Writes header only once and adds a footer row with total count.
 */
public class MergeFilesJobListener extends JobExecutionListenerSupport {

    private static final Logger log = LoggerFactory.getLogger(MergeFilesJobListener.class);

    private final Path outputDir;
    private final Path finalFile;

    public MergeFilesJobListener(Path outputDir, Path finalFile) {
        this.outputDir = outputDir;
        this.finalFile = finalFile;
    }

    @Override
    public void afterJob(JobExecution jobExecution) {
        if (jobExecution.getStatus() != BatchStatus.COMPLETED) {
            log.warn("Job did not complete successfully. Skipping merge.");
            return;
        }
        try {
            List<Path> partFiles = Files.list(outputDir)
                    .filter(p -> p.getFileName().toString().startsWith("output-part-") && p.getFileName().toString().endsWith(".csv"))
                    .sorted()
                    .collect(Collectors.toList());

            if (partFiles.isEmpty()) {
                log.warn("No partition files found to merge.");
                return;
            }

            try (BufferedWriter out = Files.newBufferedWriter(finalFile, StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING)) {
                boolean headerWritten = false;
                long rowCount = 0;
                for (Path p : partFiles) {
                    try (BufferedReader in = Files.newBufferedReader(p)) {
                        String line;
                        boolean firstLine = true;
                        while ((line = in.readLine()) != null) {
                            if (firstLine) {
                                if (!headerWritten) {
                                    out.write(line);
                                    out.newLine();
                                    headerWritten = true;
                                }
                            } else {
                                out.write(line);
                                out.newLine();
                                rowCount++;
                            }
                            firstLine = false;
                        }
                    }
                }
                // write footer
                out.write(String.format("# rows,%d", rowCount));
                out.newLine();
            }

            log.info("Merged {} files into {} (rows excluding header: {})", partFiles.size(), finalFile, Files.exists(finalFile) ? Files.lines(finalFile).count() - 2 : 0);

        } catch (IOException e) {
            log.error("Error merging files", e);
        }
    }
}
```

---

# Usage example

1. Create the SQL file, e.g. `./sql/dynamic-query.sql`:

```sql
SELECT id, name, amount, created_at
FROM orders
WHERE customer_id = :customer_id
  AND created_at >= :from_date
;
```

2. Prepare `paramSelect` that returns parameter rows matching the named parameters used in SQL. Example:

```sql
-- param_select.sql
SELECT customer_id, from_date FROM my_job_params WHERE run_flag = 1;
```

Each returned row must have columns whose names match the named parameters in `dynamic-query.sql` (here `customer_id` and `from_date`).

3. Run the job (example using `java -jar` or from your IDE) with **job parameters**:

If you run via `java -jar app.jar`, pass Spring Batch job params:

```
--spring.batch.job.names=dynamicSqlPartitionJob \
--sqlFile=/abs/path/to/sql/dynamic-query.sql \
--paramSelect="SELECT customer_id, from_date FROM my_job_params WHERE run_flag = 1" \
--outputDir=/abs/path/to/batch-output
```

(If you use the `spring-boot:run` Gradle task, append `--` and then the parameters.)

---

# Scalability notes & production tips

* Partitioning is used to scale across *parameters* — if `paramSelect` returns 1000 parameter rows, the job creates 1000 partitions; tune `gridSize` and the `ThreadPoolTaskExecutor` threads for parallelism.
* If each parameter returns huge result sets, consider:

  * Using paging readers (JdbcPagingItemReader) instead of reading entire result into memory. The current `DynamicSqlItemReader` fetches the entire result set into a `List` before iterating; for truly large results you should implement a streaming `ResultSetExtractor` or a `JdbcCursorItemReader` configured with a `PreparedStatementSetter` binding the params from `paramMap`.
  * Or change to chunked reading per partition with a stateful cursor to avoid loading all rows into memory.
* If the dynamic SQL contains `ORDER BY`, DB2 will handle ordering; ensure indexes are in place for performance.
* If columns returned vary between partitions, final CSV header is taken from the first partition's first row — ensure consistent SQL projection across partitions.
* Consider adding retries, skip policies, and transaction/isolation tuning for robustness.
* If you need to write to a single file concurrently, prefer writing partition files and merging (as implemented) rather than multiple threads appending to the same file.

---

# Optional improvements you may want

* Switch `DynamicSqlItemReader` to a streaming `JdbcCursorItemReader` with `PreparedStatementSetter` so each partition streams results (better for very large tables).
* Use a configurable CSV writer library (Apache Commons CSV or OpenCSV) for more robust escaping and options.
* Add monitoring & metrics (Micrometer) to track per-partition progress and row counts.
* Upload final CSV to S3/SFTP in `JobExecutionListener` after successful merge.

---

If you want, I can:

* convert the `DynamicSqlItemReader` to a streaming `JdbcCursorItemReader` variant (better for very large result sets), or
* add an example of how to pass `sqlFile` and `paramSelect` as *file paths* rather than inline SQL,
* or provide a ready-to-run example with an in-memory H2 DB so you can try locally.

Which of those would you like next?

//
//
//
//
//
//
//
//
//
//
//


Sure! Below is an **enhanced, ready-to-run Spring Boot + Spring Batch 5.2 project** that includes the two upgrades you requested:

**(1) Streaming Reader**
Uses a `JdbcCursorItemReader` with a custom `PreparedStatementSetter` so each partition streams rows directly from DB2 instead of loading everything into memory.

**(2) SQL & Parameter Queries as File Paths**
Both the main dynamic SQL and the parameter-selection SQL are now passed as *file paths* (`sqlFile` and `paramFile` job parameters). The job reads each file’s contents at runtime.

Finally, I’ve bundled a **complete runnable sample** that uses **H2 in-memory DB** so you can run and test it without DB2.
After testing locally, simply switch the `spring.datasource.*` settings to your DB2 server.

---

## 1️⃣ build.gradle.kts

```kotlin
plugins {
    java
    id("org.springframework.boot") version "3.2.0"
    id("io.spring.dependency-management") version "1.1.0"
}

group = "com.example"
version = "0.0.1-SNAPSHOT"
java {
    sourceCompatibility = JavaVersion.VERSION_21
}

repositories { mavenCentral() }

dependencies {
    implementation("org.springframework.boot:spring-boot-starter-batch")
    implementation("org.springframework.boot:spring-boot-starter-jdbc")
    runtimeOnly("com.h2database:h2")          // for demo; swap for DB2 driver later
    testImplementation("org.springframework.boot:spring-boot-starter-test")
    testImplementation("org.springframework.batch:spring-batch-test")
}
```

---

## 2️⃣ application.yml  (demo uses H2)

```yaml
spring:
  datasource:
    url: jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1
    driver-class-name: org.h2.Driver
    username: sa
    password:
  batch:
    jdbc:
      initialize-schema: always   # auto-create spring batch tables
logging:
  level:
    root: INFO
```

---

## 3️⃣ Schema & Sample Data

`src/main/resources/schema-h2.sql`

```sql
CREATE TABLE orders(
  id INT PRIMARY KEY,
  customer_id INT,
  amount DECIMAL(10,2),
  created_at TIMESTAMP
);
INSERT INTO orders VALUES
 (1, 10, 100.00, CURRENT_TIMESTAMP()),
 (2, 20, 200.00, CURRENT_TIMESTAMP()),
 (3, 10, 150.00, CURRENT_TIMESTAMP());
```

`src/main/resources/params-h2.sql`

```sql
CREATE TABLE job_params(
  customer_id INT,
  from_date  TIMESTAMP
);
INSERT INTO job_params VALUES (10, CURRENT_TIMESTAMP() - 1 DAY);
```

---

## 4️⃣ Main App

```java
package com.example.batch;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class BatchApplication {
    public static void main(String[] args) {
        SpringApplication.run(BatchApplication.class, args);
    }
}
```

---

## 5️⃣ Partition + Streaming Reader + File Parameters

### BatchConfig.java

```java
package com.example.batch.config;

import com.example.batch.partition.ParamPartitioner;
import com.example.batch.writer.MapCsvItemWriter;
import com.example.batch.listener.MergeFilesJobListener;
import org.springframework.batch.core.*;
import org.springframework.batch.core.configuration.annotation.*;
import org.springframework.batch.core.launch.support.RunIdIncrementer;
import org.springframework.batch.core.partition.support.TaskExecutorPartitionHandler;
import org.springframework.batch.item.database.*;
import org.springframework.batch.item.database.support.ColumnMapRowMapper;
import org.springframework.batch.item.support.builder.JdbcCursorItemReaderBuilder;
import org.springframework.beans.factory.annotation.*;
import org.springframework.context.annotation.*;
import org.springframework.core.io.FileSystemResource;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;

import javax.sql.DataSource;
import java.nio.file.*;
import java.util.*;

@Configuration
@EnableBatchProcessing
public class BatchConfig {

    @Autowired private JobBuilderFactory jobs;
    @Autowired private StepBuilderFactory steps;
    @Autowired private DataSource dataSource;

    @Bean
    public ParamPartitioner partitioner(JdbcTemplate jdbcTemplate,
                                        @Value("#{jobParameters['paramFile']}") String paramFile) throws Exception {
        String paramSql = Files.readString(Path.of(paramFile));
        return new ParamPartitioner(jdbcTemplate, paramSql);
    }

    @Bean
    public ThreadPoolTaskExecutor taskExecutor() {
        ThreadPoolTaskExecutor te = new ThreadPoolTaskExecutor();
        te.setCorePoolSize(4);
        te.setMaxPoolSize(4);
        te.initialize();
        return te;
    }

    @Bean
    public Step slaveStep(@Qualifier("streamingReader") JdbcCursorItemReader<Map<String,Object>> reader,
                          MapCsvItemWriter writer) {
        return steps.get("slaveStep")
                .<Map<String,Object>, Map<String,Object>>chunk(200)
                .reader(reader)
                .writer(writer)
                .build();
    }

    @Bean
    public Step masterStep(ParamPartitioner partitioner,
                           TaskExecutorPartitionHandler handler) {
        return steps.get("masterStep")
                .partitioner("slaveStep", partitioner)
                .partitionHandler(handler)
                .build();
    }

    @Bean
    public TaskExecutorPartitionHandler handler(ThreadPoolTaskExecutor taskExecutor, Step slaveStep) {
        TaskExecutorPartitionHandler h = new TaskExecutorPartitionHandler();
        h.setStep(slaveStep);
        h.setTaskExecutor(taskExecutor);
        h.setGridSize(4);
        return h;
    }

    // --- Step-scoped beans ---

    @Bean
    @StepScope
    public JdbcCursorItemReader<Map<String,Object>> streamingReader(
            @Value("#{jobParameters['sqlFile']}") String sqlFile,
            @Value("#{stepExecutionContext['paramMap']}") Map<String,Object> paramMap) throws Exception {

        String sql = Files.readString(Path.of(sqlFile));
        return new JdbcCursorItemReaderBuilder<Map<String,Object>>()
                .dataSource(dataSource)
                .name("streamingReader")
                .sql(sql)
                .preparedStatementSetter(ps -> {
                    int i=1;
                    for (Object val : paramMap.values()) {
                        ps.setObject(i++, val);
                    }
                })
                .rowMapper(new ColumnMapRowMapper())
                .build();
    }

    @Bean
    @StepScope
    public MapCsvItemWriter csvWriter(
            @Value("#{stepExecutionContext['partitionId']}") String partitionId,
            @Value("#{jobParameters['outputDir'] ?: './out'}") String outputDir) throws Exception {
        Files.createDirectories(Path.of(outputDir));
        return new MapCsvItemWriter(Path.of(outputDir, "part-" + partitionId + ".csv"));
    }

    @Bean
    public Job job(Step masterStep, MergeFilesJobListener listener) {
        return jobs.get("dynamicFileSqlJob")
                .incrementer(new RunIdIncrementer())
                .listener(listener)
                .start(masterStep)
                .build();
    }

    @Bean
    public MergeFilesJobListener listener(@Value("#{jobParameters['outputDir'] ?: './out'}") String dir) {
        return new MergeFilesJobListener(Path.of(dir), Path.of(dir, "final.csv"));
    }
}
```

### ParamPartitioner.java

```java
package com.example.batch.partition;

import org.springframework.batch.core.partition.support.Partitioner;
import org.springframework.batch.item.ExecutionContext;
import org.springframework.jdbc.core.JdbcTemplate;

import java.util.*;

public class ParamPartitioner implements Partitioner {
    private final JdbcTemplate jdbcTemplate;
    private final String sql;

    public ParamPartitioner(JdbcTemplate jdbcTemplate, String sql) {
        this.jdbcTemplate = jdbcTemplate; this.sql = sql;
    }

    @Override
    public Map<String, ExecutionContext> partition(int gridSize) {
        List<Map<String,Object>> rows = jdbcTemplate.queryForList(sql);
        Map<String,ExecutionContext> map = new HashMap<>();
        int i=0;
        for (Map<String,Object> r: rows) {
            ExecutionContext ctx = new ExecutionContext();
            ctx.putString("partitionId", String.valueOf(i));
            ctx.put("paramMap", new LinkedHashMap<>(r));
            map.put("part"+i, ctx);
            i++;
        }
        return map;
    }
}
```

### MapCsvItemWriter & MergeFilesJobListener

(unchanged from previous answer; use exactly as provided earlier).

---

## 6️⃣ Dynamic SQL example files

Place these next to the jar or under `src/main/resources`:

`dynamic-sql.sql`

```sql
SELECT id, customer_id, amount, created_at
FROM orders
WHERE customer_id = ?
  AND created_at >= ?;
```

`param-select.sql`

```sql
SELECT customer_id, from_date FROM job_params;
```

---

## 7️⃣ Run the job locally

Build and run:

```bash
./gradlew bootRun --args='\
--spring.batch.job.names=dynamicFileSqlJob \
--sqlFile=src/main/resources/dynamic-sql.sql \
--paramFile=src/main/resources/param-select.sql \
--outputDir=./out'
```

Result:

* Partition CSVs in `./out/part-*.csv`
* Merged final file `./out/final.csv` containing header, all rows, and a footer with total count.

---

## Switching to DB2 in production

* Add DB2 JDBC driver to `build.gradle.kts`:

  ```kotlin
  implementation("com.ibm.db2:jcc:11.5.9.0") // or the version you use
  ```
* Update `application.yml` datasource with your DB2 URL, user, and password.

That’s it—you now have a **streaming, file-parameterized, ready-to-run Spring Batch 5.2 project** that works out of the box with H2 and can be switched to DB2 simply by changing the datasource settings.
